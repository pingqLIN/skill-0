#!/usr/bin/env python3
"""
Skill-0 çµæ§‹åˆ†æå·¥å…·
åˆ†æå·²è§£æçš„ skillsï¼Œç”¢ç”Ÿçµ±è¨ˆå ±å‘Šèˆ‡æ¨¡å¼è­˜åˆ¥
"""

import json
import os
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field, asdict
from datetime import datetime


@dataclass
class ElementStats:
    """å–®ä¸€å…ƒç´ é¡å‹çš„çµ±è¨ˆ"""
    total_count: int = 0
    type_distribution: Dict[str, int] = field(default_factory=dict)
    common_patterns: List[str] = field(default_factory=list)


@dataclass 
class SkillSummary:
    """å–®ä¸€ Skill çš„æ‘˜è¦"""
    skill_id: str
    name: str
    source_file: str
    action_count: int = 0
    rule_count: int = 0
    directive_count: int = 0
    action_types: List[str] = field(default_factory=list)
    directive_types: List[str] = field(default_factory=list)
    has_flow: bool = False
    

@dataclass
class AnalysisReport:
    """å®Œæ•´åˆ†æå ±å‘Š"""
    version: str = "1.0"
    generated_at: str = ""
    total_skills: int = 0
    skills: List[SkillSummary] = field(default_factory=list)
    
    # å…¨åŸŸçµ±è¨ˆ
    action_stats: ElementStats = field(default_factory=ElementStats)
    rule_stats: ElementStats = field(default_factory=ElementStats)
    directive_stats: ElementStats = field(default_factory=ElementStats)
    
    # æ¨¡å¼åˆ†æ
    common_action_sequences: List[Dict] = field(default_factory=list)
    common_rule_patterns: List[Dict] = field(default_factory=list)


class SkillAnalyzer:
    """Skill çµæ§‹åˆ†æå™¨"""
    
    def __init__(self, parsed_dir: str):
        self.parsed_dir = Path(parsed_dir)
        self.skills: List[Dict] = []
        self.report = AnalysisReport()
        
    def load_skills(self) -> int:
        """è¼‰å…¥æ‰€æœ‰å·²è§£æçš„ skills"""
        self.skills = []
        
        for json_file in self.parsed_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    skill_data = json.load(f)
                    skill_data['_source_file'] = json_file.name
                    self.skills.append(skill_data)
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥å¤±æ•— {json_file.name}: {e}")
                
        return len(self.skills)
    
    def analyze(self) -> AnalysisReport:
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        self.report = AnalysisReport(
            generated_at=datetime.now().isoformat(),
            total_skills=len(self.skills)
        )
        
        # æ”¶é›†å™¨
        all_action_types = Counter()
        all_directive_types = Counter()
        all_rule_conditions = []
        action_sequences = []
        
        for skill in self.skills:
            summary = self._analyze_skill(skill)
            self.report.skills.append(summary)
            
            # ç´¯è¨ˆçµ±è¨ˆ
            all_action_types.update(summary.action_types)
            all_directive_types.update(summary.directive_types)
            
            # æå–åŸ·è¡Œåºåˆ—
            if 'execution_flow' in skill:
                seq = self._extract_sequence(skill['execution_flow'])
                if seq:
                    action_sequences.append(seq)
        
        # å½™æ•´å…¨åŸŸçµ±è¨ˆ
        self.report.action_stats = ElementStats(
            total_count=sum(s.action_count for s in self.report.skills),
            type_distribution=dict(all_action_types)
        )
        
        self.report.rule_stats = ElementStats(
            total_count=sum(s.rule_count for s in self.report.skills)
        )
        
        self.report.directive_stats = ElementStats(
            total_count=sum(s.directive_count for s in self.report.skills),
            type_distribution=dict(all_directive_types)
        )
        
        # åˆ†æå¸¸è¦‹æ¨¡å¼
        self.report.common_action_sequences = self._find_common_sequences(action_sequences)
        
        return self.report
    
    def _analyze_skill(self, skill: Dict) -> SkillSummary:
        """åˆ†æå–®ä¸€ skill"""
        # æ”¯æ´ v2.0 schema çµæ§‹ (decomposition.actions/rules/directives)
        decomposition = skill.get('decomposition', {})
        meta = skill.get('meta', {})
        
        actions = decomposition.get('actions', [])
        rules = decomposition.get('rules', [])
        directives = decomposition.get('directives', [])
        
        # ä¹Ÿæ”¯æ´èˆŠçš„ elements é™£åˆ—æ ¼å¼
        if not actions and not rules and not directives:
            elements = skill.get('elements', [])
            actions = [e for e in elements if e.get('type') == 'action']
            rules = [e for e in elements if e.get('type') == 'rule']
            directives = [e for e in elements if e.get('type') == 'directive']
        
        return SkillSummary(
            skill_id=meta.get('skill_id', skill.get('skill_id', 'unknown')),
            name=meta.get('name', skill.get('skill_name', 'Unknown')),
            source_file=skill.get('_source_file', ''),
            action_count=len(actions),
            rule_count=len(rules),
            directive_count=len(directives),
            action_types=[a.get('action_type', 'unknown') for a in actions],
            directive_types=[d.get('directive_type', 'unknown') for d in directives],
            has_flow='execution_flow' in decomposition or 'execution_flow' in skill
        )
    
    def _extract_sequence(self, flow: Dict) -> Optional[List[str]]:
        """å¾åŸ·è¡Œæµç¨‹æå–å‹•ä½œåºåˆ—"""
        sequence = []
        
        def traverse(node):
            if isinstance(node, dict):
                if 'step' in node:
                    # æå– step çš„å…ƒç´ é¡å‹
                    elem_id = node.get('element_ref', '')
                    if elem_id.startswith('a_'):
                        sequence.append('action')
                    elif elem_id.startswith('r_'):
                        sequence.append('rule')
                    elif elem_id.startswith('d_'):
                        sequence.append('directive')
                        
                # éè¿´è™•ç†å­ç¯€é»
                for key in ['then', 'next', 'on_true', 'on_false', 'steps']:
                    if key in node:
                        traverse(node[key])
                        
            elif isinstance(node, list):
                for item in node:
                    traverse(item)
        
        traverse(flow)
        return sequence if sequence else None
    
    def _find_common_sequences(self, sequences: List[List[str]]) -> List[Dict]:
        """æ‰¾å‡ºå¸¸è¦‹çš„å‹•ä½œåºåˆ—æ¨¡å¼"""
        # è½‰æ›ç‚ºå­—ä¸²ä»¥ä¾¿è¨ˆæ•¸
        seq_strings = ['->'.join(s) for s in sequences]
        counter = Counter(seq_strings)
        
        return [
            {"pattern": seq, "count": count}
            for seq, count in counter.most_common(10)
            if count > 1
        ]
    
    def generate_report_text(self) -> str:
        """ç”¢ç”Ÿäººé¡å¯è®€çš„å ±å‘Š"""
        r = self.report
        lines = [
            "=" * 60,
            "ğŸ“Š Skill-0 çµæ§‹åˆ†æå ±å‘Š",
            "=" * 60,
            f"ç”¢ç”Ÿæ™‚é–“: {r.generated_at}",
            f"åˆ†æ Skills æ•¸é‡: {r.total_skills}",
            "",
            "ğŸ“ˆ å…ƒç´ çµ±è¨ˆ",
            "-" * 40,
            f"  Action ç¸½æ•¸: {r.action_stats.total_count}",
            f"  Rule ç¸½æ•¸: {r.rule_stats.total_count}",
            f"  Directive ç¸½æ•¸: {r.directive_stats.total_count}",
            "",
        ]
        
        # Action é¡å‹åˆ†å¸ƒ
        if r.action_stats.type_distribution:
            lines.append("ğŸ¬ Action é¡å‹åˆ†å¸ƒ:")
            for atype, count in sorted(r.action_stats.type_distribution.items(), 
                                       key=lambda x: -x[1]):
                lines.append(f"  - {atype}: {count}")
            lines.append("")
        
        # Directive é¡å‹åˆ†å¸ƒ
        if r.directive_stats.type_distribution:
            lines.append("ğŸ“Œ Directive é¡å‹åˆ†å¸ƒ:")
            for dtype, count in sorted(r.directive_stats.type_distribution.items(),
                                       key=lambda x: -x[1]):
                lines.append(f"  - {dtype}: {count}")
            lines.append("")
        
        # å„ Skill æ‘˜è¦
        lines.append("ğŸ“‹ å„ Skill æ‘˜è¦")
        lines.append("-" * 40)
        for s in r.skills:
            lines.append(f"  [{s.skill_id}] {s.name}")
            lines.append(f"    Actions: {s.action_count}, Rules: {s.rule_count}, Directives: {s.directive_count}")
            lines.append(f"    æœ‰åŸ·è¡Œæµç¨‹: {'âœ“' if s.has_flow else 'âœ—'}")
            lines.append("")
        
        # å¸¸è¦‹æ¨¡å¼
        if r.common_action_sequences:
            lines.append("ğŸ”„ å¸¸è¦‹åŸ·è¡Œåºåˆ—æ¨¡å¼")
            lines.append("-" * 40)
            for p in r.common_action_sequences:
                lines.append(f"  {p['pattern']} (å‡ºç¾ {p['count']} æ¬¡)")
            lines.append("")
        
        lines.append("=" * 60)
        return "\n".join(lines)
    
    def save_report(self, output_path: str):
        """å„²å­˜ JSON å ±å‘Š"""
        # è½‰æ› dataclass ç‚º dict
        report_dict = {
            "version": self.report.version,
            "generated_at": self.report.generated_at,
            "total_skills": self.report.total_skills,
            "summary": {
                "action_count": self.report.action_stats.total_count,
                "rule_count": self.report.rule_stats.total_count,
                "directive_count": self.report.directive_stats.total_count,
            },
            "action_type_distribution": self.report.action_stats.type_distribution,
            "directive_type_distribution": self.report.directive_stats.type_distribution,
            "skills": [asdict(s) for s in self.report.skills],
            "common_patterns": {
                "action_sequences": self.report.common_action_sequences
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Skill-0 çµæ§‹åˆ†æå·¥å…·')
    parser.add_argument('--parsed-dir', '-p', default='parsed',
                        help='å·²è§£æ skills çš„ç›®éŒ„ (é è¨­: parsed)')
    parser.add_argument('--output', '-o', default='analysis/report.json',
                        help='è¼¸å‡ºå ±å‘Šè·¯å¾‘ (é è¨­: analysis/report.json)')
    parser.add_argument('--text', '-t', action='store_true',
                        help='åŒæ™‚è¼¸å‡ºæ–‡å­—å ±å‘Š')
    
    args = parser.parse_args()
    
    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # åŸ·è¡Œåˆ†æ
    analyzer = SkillAnalyzer(args.parsed_dir)
    
    print(f"ğŸ“‚ è¼‰å…¥ skills å¾: {args.parsed_dir}")
    count = analyzer.load_skills()
    print(f"âœ“ è¼‰å…¥ {count} å€‹ skills")
    
    print("ğŸ” åŸ·è¡Œåˆ†æ...")
    analyzer.analyze()
    
    # è¼¸å‡º
    analyzer.save_report(args.output)
    print(f"âœ“ JSON å ±å‘Šå·²å„²å­˜: {args.output}")
    
    if args.text:
        text_report = analyzer.generate_report_text()
        text_path = output_path.with_suffix('.txt')
        with open(text_path, 'w', encoding='utf-8') as f:
            f.write(text_report)
        print(f"âœ“ æ–‡å­—å ±å‘Šå·²å„²å­˜: {text_path}")
    
    # é¡¯ç¤ºæ‘˜è¦
    print("\n" + analyzer.generate_report_text())


if __name__ == '__main__':
    main()
